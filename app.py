#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
DataNeuron Streamlit Frontend Application
=========================================

Complete Streamlit interface for DataNeuron project following Clean Code and SOLID principles.
Integrates all backend components with proper error handling and user feedback.

Architecture:
- Modular functions with single responsibilities
- Singleton pattern for backend components
- Proper async handling and user feedback
- Session-based data isolation
"""

import asyncio
import hashlib
import uuid
import streamlit as st
from datetime import datetime
from pathlib import Path
from typing import List, Dict, Any, Optional
from contextlib import contextmanager
import sys
from config.settings import UPLOADS_DIR

# Add project root to Python path
PROJECT_ROOT = Path(__file__).resolve().parent
sys.path.insert(0, str(PROJECT_ROOT))

# Import backend components with exact names from requirements
from core.document_processor import DocumentProcessor, Document
from core.chunker import TextChunker, Chunk
from core.embedder import Embedder
from core.vector_store import VectorStore
from core.session_manager import SessionManager, SessionData
from core.llm_agent import LLMAgent, CoTSession
from utils.logger import logger
from pydantic import BaseModel
from core.tool_manager import ToolManager

@contextmanager
def processing_context():
    """Context manager for safe processing state management."""
    st.session_state.is_processing = True
    try:
        yield
    finally:
        st.session_state.is_processing = False

def get_session_documents_safely(session_id: str) -> List[Any]:
    """Safe wrapper for getting session documents."""
    try:
        return st.session_state.session_manager.get_session_documents(session_id)
    except Exception as e:
        logger.warning(f"Could not fetch documents: {e}")
        return []

def validate_uploaded_file(uploaded_file) -> tuple[bool, str]:
    """Validate uploaded file before processing."""
    # Size check (max 50MB)
    max_size = 50 * 1024 * 1024
    file_size = len(uploaded_file.getbuffer())
    if file_size > max_size:
        return False, f"File too large: {file_size/(1024*1024):.1f}MB > 50MB"
    
    # Type check
    allowed_types = ['.pdf', '.docx', '.txt']
    file_ext = Path(uploaded_file.name).suffix.lower()
    if file_ext not in allowed_types:
        return False, f"Unsupported file type: {file_ext}"
    
    # Name validation
    if not uploaded_file.name or len(uploaded_file.name.strip()) == 0:
        return False, "Invalid file name"
    
    return True, "Valid"

def save_uploaded_file_safely(uploaded_file, temp_path: Path, chunk_size: int = 8192) -> bool:
    """Save uploaded file with memory optimization."""
    try:
        temp_path.parent.mkdir(exist_ok=True)
        
        with open(temp_path, "wb") as f:
            uploaded_file.seek(0)
            while True:
                chunk = uploaded_file.read(chunk_size)
                if not chunk:
                    break
                f.write(chunk)
        
        return True
    except Exception as e:
        logger.exception(f"File save failed: {e}")
        return False

def safe_cleanup_temp_file(file_path: Path):
    """Safely cleanup temporary files."""
    try:
        if file_path.exists():
            file_path.unlink()
            logger.debug(f"Cleaned up temp file: {file_path}")
    except Exception as e:
        logger.warning(f"Could not cleanup temp file {file_path}: {e}")

def initialize_session_state():
    """Initialize all required Streamlit session state variables."""
    logger.info("Initializing Streamlit session state")
    
    # Generate unique session ID
    if 'session_id' not in st.session_state:
        st.session_state.session_id = f"user_{uuid.uuid4().hex[:8]}"
        logger.info(f"Generated session ID: {st.session_state.session_id}")
    
    # Initialize state variables
    if 'is_processing' not in st.session_state:
        st.session_state.is_processing = False
    
    if 'chat_history' not in st.session_state:
        st.session_state.chat_history = []
    
    # Initialize backend singletons with error handling
    try:
        if 'document_processor' not in st.session_state:
            st.session_state.document_processor = DocumentProcessor()
            logger.info("DocumentProcessor initialized")
        
        if 'text_chunker' not in st.session_state:
            st.session_state.text_chunker = TextChunker()
            logger.info("TextChunker initialized")
        
        if 'embedder' not in st.session_state:
            st.session_state.embedder = Embedder()
            logger.info("Embedder initialized")
        
        if 'vector_store' not in st.session_state:
            st.session_state.vector_store = VectorStore()
            logger.info("VectorStore initialized")
        
        if 'session_manager' not in st.session_state:
            st.session_state.session_manager = SessionManager()
            logger.info("SessionManager initialized")

        if 'tool_manager' not in st.session_state:
            st.session_state.tool_manager = ToolManager(session_manager=st.session_state.session_manager)
            logger.info("ToolManager initialized with injected SessionManager")
        
        if 'llm_agent' not in st.session_state:
            st.session_state.llm_agent = LLMAgent(
                tool_manager=st.session_state.tool_manager,
                session_manager=st.session_state.session_manager
            )
            logger.info("LLMAgent initialized with shared managers")
            
    except Exception as e:
        logger.exception(f"Backend initialization failed: {e}")
        st.error(f"Backend initialization failed: {str(e)}")
        st.stop()
    
    logger.success("Session state initialization completed")

def render_sidebar():
    """Fixed sidebar with safe document fetching."""
    with st.sidebar:
        st.header("üìÑ Documents")
        
        # Get documents safely
        uploaded_documents = get_session_documents_safely(st.session_state.session_id)
        
        # Display uploaded documents
        if uploaded_documents:
            st.write(f"**{len(uploaded_documents)} documents loaded:**")
            
            for doc in uploaded_documents:
                with st.expander(f"{doc.file_name} ({doc.file_size_mb:.1f}MB)"):
                    st.write(f"**Processed:** {doc.processed_at[:19]}")
                    st.write(f"**Collection:** {doc.vector_collection_name}")
                    if doc.content_preview:
                        st.write(f"**Preview:** {doc.content_preview[:100]}...")
                    
                    if st.button(
                        "Remove", 
                        key=f"remove_{doc.file_hash}",
                        disabled=st.session_state.is_processing
                    ):
                        remove_document_from_session(doc.file_hash)
        else:
            st.info("No documents uploaded yet")
        
        # System Status
        st.header("System Status")
        with st.expander("Backend Components"):
            components = [
                ("Document Processor", 'document_processor'),
                ("Text Chunker", 'text_chunker'),
                ("Embedder", 'embedder'),
                ("Vector Store", 'vector_store'),
                ("Session Manager", 'session_manager'),
                ("Tool Manager", 'tool_manager'),
                ("LLM Agent", 'llm_agent')
            ]
            
            for name, key in components:
                status = "‚úÖ" if key in st.session_state else "‚ùå"
                st.write(f"{status} {name}")
        
        with st.expander("Session Information"):
            st.write(f"**Session ID:** `{st.session_state.session_id}`")
            st.write(f"**Chat History:** {len(st.session_state.chat_history)} messages")
            processing_status = "üîÑ Processing" if st.session_state.is_processing else "‚úÖ Ready"
            st.write(f"**Status:** {processing_status}")
        
        # Clear session button
        st.header("Session Management")
        if st.button(
            "Clear All Data", 
            type="secondary",
            disabled=st.session_state.is_processing
        ):
            clear_all_session_data()

def render_chat_interface():
    """Fixed chat interface with proper state management."""
    st.header("üí¨ Chat Interface")

    # Display chat history
    for message in st.session_state.get('chat_history', []):
        with st.chat_message(message["role"]):
            if message.get("is_clarification_request", False):
                st.markdown("ü§î **I need clarification:**")
                st.markdown(message["content"])
                st.info("üí° Please provide more details to help me assist you better.")
            else:
                st.markdown(message["content"])
            
            if "cot_session" in message:
                with st.expander("üß† View Agent's Reasoning"):
                    display_cot_visualization(message["cot_session"])

    # ============================================================================
    # HEDEFLENMƒ∞≈û DOK√úMAN SORGULAMA - DOK√úMAN SE√áƒ∞M Bƒ∞LE≈ûENƒ∞
    # ============================================================================
    
    # Y√ºkl√º dok√ºmanlarƒ± al
    uploaded_documents = get_session_documents_safely(st.session_state.session_id)
    
    if uploaded_documents:
        # Dok√ºman isimlerini listele
        doc_filenames = [doc.file_name for doc in uploaded_documents]
        
        st.markdown("### üìã Dok√ºman Se√ßimi")
        st.markdown("Sorgunuz i√ßin hangi dok√ºmanlarƒ± analiz etmek istiyorsunuz?")
        
        # Multiselect bile≈üeni
        selected_files = st.multiselect(
            "Sorgunuz i√ßin dok√ºman se√ßin (birden √ßok se√ßim yapabilirsiniz):",
            options=doc_filenames,
            default=st.session_state.get('last_selected_files', doc_filenames),  # Varsayƒ±lan: t√ºm dosyalar
            key="document_selector",
            help="Se√ßili dok√ºmanlar √ºzerinde analiz yapƒ±lacak. Hi√ß se√ßim yapmazsanƒ±z size sorulacak.",
            disabled=st.session_state.is_processing
        )
        
        # Se√ßimi session state'e kaydet
        st.session_state.last_selected_files = selected_files
        
        # Se√ßim durumu g√∂sterimi
        if selected_files:
            if len(selected_files) == len(doc_filenames):
                st.success(f"‚úÖ T√ºm dok√ºmanlar se√ßili ({len(selected_files)} dok√ºman)")
            else:
                st.info(f"üìë {len(selected_files)} dok√ºman se√ßili: {', '.join(selected_files)}")
        else:
            st.warning("‚ö†Ô∏è Hi√ß dok√ºman se√ßilmedi. Sorguyu g√∂nderdiƒüinizde size dok√ºman se√ßimi sorulacak.")
        
        st.markdown("---")
    
    else:
        selected_files = []
        st.info("üìÑ Hen√ºz y√ºklenmi≈ü dok√ºman bulunmuyor. √ñnce 'Upload Documents' sekmesinden dok√ºman y√ºkleyin.")
    
    # ============================================================================
    # HEDEFLENMƒ∞≈û DOK√úMAN SORGULAMA Bƒ∞LE≈ûENƒ∞ SONU
    # ============================================================================
    
    # ============================================================================
    # AKILLI ƒ∞NTERNET ARAMA ENTEGRASYONUc
    # ============================================================================
    
    st.markdown("### üåê ƒ∞nternet Arama")
    
    # Web search toggle
    web_search_enabled = st.toggle(
        "ƒ∞nternette Ara",
        value=st.session_state.get('web_search_enabled', False),
        key="web_search_toggle",
        help="Etkinle≈ütirildiƒüinde, ajan y√ºkl√º dok√ºmanlarƒ±nƒ±za ek olarak internetten g√ºncel bilgi arayabilir.",
        disabled=st.session_state.is_processing
    )
    
    # Update session state
    st.session_state.web_search_enabled = web_search_enabled
    
    # ### YENƒ∞ VE DOƒûRU DURUM G√ñSTERƒ∞Mƒ∞ ###
    if web_search_enabled:
        st.success("‚úÖ ƒ∞nternet aramasƒ± etkin - Ajan hem dok√ºmanlarƒ±nƒ±zƒ± hem de g√ºncel web bilgilerini kullanabilir.")
        st.info("üí° **ƒ∞pucu:** ƒ∞nternet aramasƒ± √∂zellikle g√ºncel haberler, ≈üirket bilgileri ve yeni geli≈ümeler gibi konularda faydalƒ±dƒ±r.")
    else:
        st.info("üìÑ Sadece y√ºkl√º dok√ºmanlar - Ajan yalnƒ±zca bu oturumdaki dok√ºmanlarƒ± kullanacak.")
        
    st.markdown("---")
    
    # ============================================================================
    # AKILLI ƒ∞NTERNET ARAMA ENTEGRASYONU SONU  
    # ============================================================================

    # Handle user input - FIXED VERSION (no infinite loop)
    # Dynamic chat input placeholder based on capabilities
    if web_search_enabled:
        chat_placeholder = "Ask about your documents or search the web for current information..."
    else:
        chat_placeholder = "Ask about your documents..."
    
    if prompt := st.chat_input(chat_placeholder, key="chat_widget", disabled=st.session_state.is_processing):
        # KULLANICI MESAJINI ANINDA CHAT GE√áMƒ∞≈ûƒ∞NE EKLE
        st.session_state.chat_history.append({"role": "user", "content": prompt})
        
        # --- YENƒ∞ KAPSAMLI ZENGƒ∞NLE≈ûTƒ∞RME MANTIƒûI ---
        final_prompt_for_agent = prompt
        
        # Adƒ±m 1: Web aramasƒ± a√ßƒ±k mƒ±? A√ßƒ±ksa, web arama komutu ekle.
        if web_search_enabled:
            web_prefix = "Use the web search tool to answer the following: "
            final_prompt_for_agent = f"{web_prefix}{prompt}"
            logger.info("Web search is enabled. Prepended web search command to the prompt.")

        # Adƒ±m 2: Dosya se√ßili mi? Se√ßiliyse, dosya bilgisini de ekle.
        if selected_files:
            file_names_str = ", ".join([f"'{f}'" for f in selected_files])
            # √ñNEMLƒ∞: Web aramasƒ± zaten eklenmi≈ü olabilir, bu y√ºzden final_prompt_for_agent'ƒ± kullan.
            file_prefix = f"Using the following document(s) as primary context: {file_names_str}. "
            final_prompt_for_agent = f"{file_prefix}{final_prompt_for_agent}"
            logger.info("Selected files found. Prepended document context to the prompt.")

        logger.info(f"Final prompt for agent: {final_prompt_for_agent}")
        # --- YENƒ∞ MANTIK SONU ---
        
        # Process LLM response immediately with context manager
        with processing_context():
            with st.chat_message("assistant"):
                with st.spinner("üß† Agent is thinking..."):
                    try:
                        # ============================================================================
                        # SOHBETEDƒ∞LEN HAFIZA - SON 6 MESAJƒ∞ AL (3 √ßift: kullanƒ±cƒ± + asistan)
                        # ============================================================================
                        
                        # Get recent chat history for agent (excluding current message)
                        full_history = st.session_state.chat_history[:-1]  # Son mesaj hari√ß
                        
                        # Son 20 mesajƒ± al (daha uzun hafƒ±za penceresi i√ßin)
                        # Bu ≈üekilde maksimum 10 soru-cevap √ßifti hafƒ±zada tutulur
                        max_history_length = 20
                        if len(full_history) > max_history_length:
                            history_for_agent = full_history[-max_history_length:]
                            logger.info(f"üß† Using last {len(history_for_agent)} messages from chat history for conversational memory")
                        else:
                            history_for_agent = full_history
                            if history_for_agent:
                                logger.info(f"üß† Using all {len(history_for_agent)} messages from chat history for conversational memory")
                        
                        # Execute agent with selected filenames and web search capability
                        cot_session = asyncio.run(
                            st.session_state.llm_agent.execute_with_cot(
                                query=final_prompt_for_agent,
                                session_id=st.session_state.session_id,
                                chat_history=history_for_agent,
                                selected_filenames=selected_files,  # YENƒ∞: Hedeflenmi≈ü dok√ºman sorgulama
                                allow_web_search=web_search_enabled  # YENƒ∞: Akƒ±llƒ± ƒ∞nternet Arama
                            )
                        )
                        
                        # Check for clarification request - √áifte kontrol sistemi
                        is_asking_clarification = False
                        clarification_question = None
                        
                        # Birinci kontrol: CoTSession metadata'dan kontrol et
                        if cot_session.metadata.get("clarification_requested", False):
                            is_asking_clarification = True
                            clarification_question = cot_session.metadata.get("clarification_question", "Could you please clarify?")
                            logger.info("üîç Clarification detected via CoTSession metadata")
                        
                        # ƒ∞kinci kontrol: Son ara√ß √ßaƒürƒ±sƒ±ndan kontrol et (fallback)
                        elif cot_session.tool_executions:
                            last_tool_execution = cot_session.tool_executions[-1]
                            if last_tool_execution.tool_name == "ask_user_for_clarification":
                                is_asking_clarification = True
                                clarification_question = last_tool_execution.arguments.get("question", "Could you please clarify?")
                                logger.info("üîç Clarification detected via last tool execution")
                        
                        # Determine response content
                        if is_asking_clarification:
                            response_content = clarification_question
                            response_metadata = {"is_clarification_request": True}
                        else:
                            response_content = cot_session.final_answer
                            response_metadata = {}
                        
                        # Display response immediately
                        if response_metadata.get("is_clarification_request"):
                            st.markdown("ü§î **I need clarification:**")
                            st.markdown(response_content)
                            
                            # ============================================================================
                            # AKILLI Y√ñNLENDƒ∞RME MANTƒûI - CLARIFICATION ƒ∞√áƒ∞N √ñNERƒ∞LER
                            # ============================================================================
                            
                            # Dokuman durumunu kontrol et
                            has_documents = len(uploaded_documents) > 0
                            web_search_available = not web_search_enabled
                            
                            # Akƒ±llƒ± √∂neriler olu≈ütur
                            suggestions = []
                            
                            if not has_documents:
                                suggestions.append("üìÅ **Dok√ºman y√ºkleyin:** 'Upload Documents' sekmesinden ilgili d√∂k√ºmanlarƒ± y√ºkleyerek daha ayrƒ±ntƒ±lƒ± analiz alabilirsiniz.")
                            
                            if web_search_available and has_documents:
                                suggestions.append("üåê **ƒ∞nternet aramasƒ±nƒ± etkinle≈ütirin:** Yukarƒ±daki 'ƒ∞nternette Ara' se√ßeneƒüini a√ßarak g√ºncel bilgilere eri≈üebilirsiniz.")
                            
                            if not has_documents and web_search_available:
                                suggestions.append("üåê **ƒ∞nternet aramasƒ±nƒ± deneyin:** 'ƒ∞nternette Ara' se√ßeneƒüini etkinle≈ütirerek g√ºncel web bilgilerine ula≈üabilirsiniz.")
                            
                            if has_documents and len(selected_files) == 0:
                                suggestions.append("üéØ **Dok√ºman se√ßin:** Yukarƒ±daki listeden analiz edilecek spesifik dok√ºmanlarƒ± se√ßin.")
                            
                            if has_documents and web_search_enabled:
                                suggestions.append("üí¨ **Sorunuzu detaylandƒ±rƒ±n:** Hangi spesifik bilgiyi aradƒ±ƒüƒ±nƒ±zƒ± daha a√ßƒ±k belirtin.")
                            
                            # √ñnerileri g√∂ster
                            if suggestions:
                                st.warning("**üí° Bu √∂neriler size yardƒ±mcƒ± olabilir:**")
                                for suggestion in suggestions:
                                    st.markdown(f"‚Ä¢ {suggestion}")
                            else:
                                st.info("üí° Please provide more details to help me assist you better.")
                            
                            # ============================================================================
                            # AKILLI Y√ñNLENDƒ∞RME SONU
                            # ============================================================================
                        else:
                            st.markdown(response_content)
                        
                        # Show reasoning if available
                        with st.expander("üß† View Agent's Reasoning"):
                            display_cot_visualization(cot_session)
                        
                        # Add to chat history
                        st.session_state.chat_history.append({
                            "role": "assistant",
                            "content": response_content,
                            "cot_session": cot_session,
                            **response_metadata
                        })
                        
                        # Ekranƒ± yenile ki hem soru hem cevap g√∂r√ºns√ºn
                        st.rerun()
                        
                    except Exception as e:
                        logger.exception(f"LLM processing failed: {e}")
                        error_msg = f"An error occurred: {e}"
                        st.error(error_msg)
                        st.session_state.chat_history.append({"role": "assistant", "content": error_msg})

def process_document_pipeline(uploaded_file) -> bool:
    """Document processing pipeline with persistent file storage."""
    logger.info(f"Starting pipeline for: {uploaded_file.name}")
    
    # Validate file first
    is_valid, validation_msg = validate_uploaded_file(uploaded_file)
    if not is_valid:
        st.error(f"‚ùå {validation_msg}")
        return False
    
    persistent_file_path = None
    try:
        # Create session-specific upload directory
        session_upload_dir = UPLOADS_DIR / st.session_state.session_id
        session_upload_dir.mkdir(parents=True, exist_ok=True)
        
        # Save file to persistent location
        persistent_file_path = session_upload_dir / uploaded_file.name
        
        if not save_uploaded_file_safely(uploaded_file, persistent_file_path):
            st.error("‚ùå Failed to save uploaded file!")
            return False
        
        # Step 1: Process document
        with st.status("üìñ Step 1/5: Processing document...", expanded=True) as status:
            document = st.session_state.document_processor.process(str(persistent_file_path))
            if not document:
                st.error("‚ùå Document processing failed!")
                return False
            
            # Enhanced content validation - UPDATED FOR NEW FORMAT
            # Combine all page texts into a single string for validation
            if isinstance(document.content, list):
                # New format: List[{"page_number": int, "text": str}]
                full_text_content = "\n".join(page.get("text", "") for page in document.content)
            else:
                # Legacy format: single string (backward compatibility)
                full_text_content = document.content
            
            content_stripped = full_text_content.strip()
            if not content_stripped or len(content_stripped) < 10:
                st.error("‚ùå Document appears to be empty or contains no readable text!")
                logger.warning(f"Document has insufficient content: {len(content_stripped)} meaningful characters")
                return False
            
            # Check content quality
            non_whitespace_chars = len([c for c in full_text_content if not c.isspace()])
            total_chars = len(full_text_content)
            if total_chars > 0 and (non_whitespace_chars / total_chars) < 0.1:
                st.error("‚ùå Document contains mostly whitespace - may be corrupted!")
                logger.warning(f"Document is {((total_chars - non_whitespace_chars) / total_chars * 100):.1f}% whitespace")
                return False
            
            status.update(label="‚úÖ Document processed", state="complete")
            logger.info(f"Document processed: {len(full_text_content)} chars, {non_whitespace_chars} non-whitespace")
            
            # Additional logging for new format
            if isinstance(document.content, list):
                logger.info(f"Document structure: {len(document.content)} pages with page numbers")
            else:
                logger.info("Document structure: legacy single-text format")
        
        # Step 2: Create chunks
        with st.status("‚úÇÔ∏è Step 2/5: Creating chunks...", expanded=True) as status:
            chunks = st.session_state.text_chunker.create_chunks(document)
            if not chunks or len(chunks) == 0:
                st.error("‚ùå Text chunking failed - no meaningful chunks could be created!")
                logger.error(f"TextChunker returned {len(chunks) if chunks else 0} chunks")
                return False
            
            status.update(label="‚úÖ Chunks created", state="complete")
            logger.info(f"Created {len(chunks)} chunks")
        
        # Step 3: Create embeddings
        with st.status("üß† Step 3/5: Creating embeddings...", expanded=True) as status:
            embeddings = st.session_state.embedder.create_embeddings(chunks)
            if not embeddings:
                st.error("‚ùå Embedding creation failed!")
                return False
            
            status.update(label="‚úÖ Embeddings created", state="complete")
            logger.info(f"Created {len(embeddings)} embeddings")
        
        # Step 4: Store in vector database
        with st.status("üíæ Step 4/5: Storing in vector database...", expanded=True) as status:
            collection_name = f"{st.session_state.session_id}_{hashlib.md5(uploaded_file.name.encode()).hexdigest()[:8]}"
            
            success = st.session_state.vector_store.add_chunks(collection_name, chunks, embeddings)
            if not success:
                st.error("‚ùå Vector storage failed!")
                return False
            
            status.update(label="‚úÖ Stored in vector database", state="complete")
            logger.info(f"Stored in collection: {collection_name}")
        
        # Step 5: Save to session
        with st.status("üìù Step 5/5: Saving to session...", expanded=True) as status:
            file_hash = hashlib.sha256(uploaded_file.getbuffer()).hexdigest()
            
            session_data = SessionData(
                file_hash=file_hash,
                file_name=uploaded_file.name,
                file_path=str(persistent_file_path),
                processed_at=datetime.now().isoformat(),
                vector_collection_name=collection_name,
                document_metadata=document.metadata,
                file_size_mb=len(uploaded_file.getbuffer()) / (1024 * 1024),
                content_preview=full_text_content[:200]
            )
            
            session_success = st.session_state.session_manager.add_document_to_session(
                st.session_state.session_id, session_data
            )
            
            if not session_success:
                st.error("‚ùå Session save failed!")
                return False
            
            status.update(label="‚úÖ Processing completed!", state="complete")
            logger.success(f"Pipeline completed: {uploaded_file.name}")
        
        return True
        
    except Exception as e:
        logger.exception(f"Pipeline failed for {uploaded_file.name}: {e}")
        st.error(f"‚ùå Processing error: {str(e)}")
        return False
    
    finally:
        # Note: Files are now stored persistently, no cleanup needed
        logger.info(f"File stored persistently at: {persistent_file_path}")

def handle_file_uploads(uploaded_files):
    """Fixed file upload handler with validation."""
    if not uploaded_files:
        return
    
    logger.info(f"Processing {len(uploaded_files)} files")
    
    with processing_context():
        successful = 0
        failed = 0
        
        try:
            for uploaded_file in uploaded_files:
                st.write(f"### üìÅ Processing: {uploaded_file.name}")
                
                # Validate before processing
                is_valid, validation_msg = validate_uploaded_file(uploaded_file)
                if not is_valid:
                    st.error(f"‚ùå Validation failed: {validation_msg}")
                    failed += 1
                    continue
                
                # Check for duplicates safely
                file_hash = hashlib.sha256(uploaded_file.getbuffer()).hexdigest()
                uploaded_documents = get_session_documents_safely(st.session_state.session_id)
                existing = any(doc.file_hash == file_hash for doc in uploaded_documents)
                
                if existing:
                    st.warning(f"Already processed: {uploaded_file.name}")
                    continue
                
                # Process file
                if process_document_pipeline(uploaded_file):
                    successful += 1
                    st.success(f"‚úÖ {uploaded_file.name} processed!")
                else:
                    failed += 1
                    st.error(f"‚ùå Failed: {uploaded_file.name}")
            
            # Show summary
            if successful > 0 or failed > 0:
                st.write("---")
                col1, col2 = st.columns(2)
                with col1:
                    st.metric("‚úÖ Successful", successful)
                with col2:
                    st.metric("‚ùå Failed", failed)
        
        except Exception as e:
            logger.exception(f"Upload handling failed: {e}")
            st.error(f"‚ùå Upload failed: {str(e)}")

def display_cot_visualization(cot_session: CoTSession):
    """Displays CoT process. Assumes it's already inside an expander."""
    if not cot_session:
        return
    
    try:
        # Overview metrics
        st.markdown("---")
        col1, col2, col3 = st.columns(3)
        col1.metric("Reasoning Steps", len(cot_session.reasoning_steps))
        col2.metric("Tool Executions", len(cot_session.tool_executions))
        col3.metric("Execution Time", f"{cot_session.total_execution_time:.2f}s")
        st.markdown("---")

        # Complexity Analysis
        if hasattr(cot_session, 'complexity_analysis') and cot_session.complexity_analysis:
            st.subheader("üìä Query Complexity Analysis")
            complexity = cot_session.complexity_analysis
            st.write(f"**Level:** `{complexity.complexity.value}` | **Confidence:** {complexity.confidence:.1%}")
            st.caption(f"Reasoning: {complexity.reasoning}")

        # Tool Executions
        if cot_session.tool_executions:
            st.subheader("üõ†Ô∏è Tool Executions")
            for exec in cot_session.tool_executions:
                success_icon = "‚úÖ" if exec.success else "‚ùå"
                with st.container(border=True):
                    st.markdown(f"{success_icon} **Tool:** `{exec.tool_name}` ({exec.execution_time:.2f}s)")
                    st.caption("Arguments:")
                    st.json(exec.arguments)
                    st.caption("Result:")
                    result_data = exec.result
                    if isinstance(result_data, BaseModel):
                        st.json(result_data.model_dump())
                    elif isinstance(result_data, dict):
                        st.json(result_data)
                    else:
                        st.code(str(result_data), language=None)
    except Exception as e:
        logger.exception(f"CoT visualization error: {e}")
        st.error(f"Could not display full reasoning: {e}")

def remove_document_from_session(file_hash: str):
    """Remove document from session and all associated data."""
    logger.info(f"Removing document: {file_hash[:16]}...")
    
    try:
        # Find document using SessionManager
        uploaded_documents = get_session_documents_safely(st.session_state.session_id)
            
        doc_to_remove = None
        for doc in uploaded_documents:
            if doc.file_hash == file_hash:
                doc_to_remove = doc
                break
        
        if not doc_to_remove:
            st.error("Document not found!")
            return
        
        # Remove using exact method signatures
        session_success = st.session_state.session_manager.remove_document_from_session(
            st.session_state.session_id, file_hash
        )
        
        vector_success = st.session_state.vector_store.delete_collection(
            doc_to_remove.vector_collection_name
        )
        
        if session_success and vector_success:
            st.success(f"‚úÖ {doc_to_remove.file_name} removed!")
            logger.success(f"Document removed: {doc_to_remove.file_name}")
        else:
            st.warning("‚ö†Ô∏è Document partially removed")
        
        st.rerun()
        
    except Exception as e:
        logger.exception(f"Remove document error: {e}")
        st.error(f"‚ùå Remove failed: {str(e)}")

def clear_all_session_data():
    """Clear all session data."""
    logger.info("Clearing all session data")
    
    try:
        # Get documents from SessionManager before clearing
        uploaded_documents = get_session_documents_safely(st.session_state.session_id)
        
        # Clear vector collections
        for doc in uploaded_documents:
            try:
                st.session_state.vector_store.delete_collection(doc.vector_collection_name)
            except Exception as e:
                logger.warning(f"Collection delete failed: {e}")
        
        # Clear session manager
        st.session_state.session_manager.clear_session(st.session_state.session_id)
        
        # Clear local state
        st.session_state.chat_history = []
        st.session_state.is_processing = False
        
        st.success("‚úÖ All data cleared!")
        logger.success("Session cleared")
        st.rerun()
        
    except Exception as e:
        logger.exception(f"Clear session error: {e}")
        st.error(f"‚ùå Clear failed: {str(e)}")

def main():
    """Fixed main function with better error handling."""
    st.set_page_config(
        page_title="DataNeuron Assistant",
        page_icon="üß†",
        layout="wide",
        initial_sidebar_state="expanded"
    )
    
    try:
        # Initialize
        initialize_session_state()
        
        # Header and styling
        st.markdown("""
        <style>
            .main-header { text-align: center; padding: 1rem 0; margin-bottom: 2rem; }
            .status-indicator { display: inline-block; width: 10px; height: 10px; border-radius: 50%; margin-right: 8px; }
            .status-ready { background-color: #28a745; }
            .status-processing { background-color: #ffc107; }
        </style>
        """, unsafe_allow_html=True)
        
        # Header with status
        st.markdown('<div class="main-header">', unsafe_allow_html=True)
        st.title("üß† DataNeuron Assistant")
        st.markdown("*AI-powered document analysis with Chain of Thought reasoning*")
        
        if st.session_state.is_processing:
            st.markdown('<span class="status-indicator status-processing"></span>**Processing...**', unsafe_allow_html=True)
        else:
            st.markdown('<span class="status-indicator status-ready"></span>**Ready**', unsafe_allow_html=True)
        
        st.markdown('</div>', unsafe_allow_html=True)
        
        # Sidebar
        render_sidebar()
        
        # Main tabs
        chat_tab, upload_tab = st.tabs(["üí¨ Chat", "üìÅ Upload Documents"])
        
        with upload_tab:
            st.header("üìÅ Document Upload & Processing")
            st.markdown("Upload PDF, DOCX, or TXT documents (max 50MB each):")
            
            uploaded_files = st.file_uploader(
                "Select documents:",
                type=['pdf', 'docx', 'txt'],
                accept_multiple_files=True,
                disabled=st.session_state.is_processing
            )
            
            if st.button(
                "üì§ Upload and Process", 
                type="primary",
                disabled=st.session_state.is_processing or not uploaded_files
            ):
                handle_file_uploads(uploaded_files)
            
            if st.session_state.is_processing:
                st.info("üîÑ Processing in progress...")
        
        with chat_tab:
            # Render chat (fixed version handles everything internally)
            render_chat_interface()
            
            # Help messages
            uploaded_documents = get_session_documents_safely(st.session_state.session_id)
            doc_count = len(uploaded_documents)
            
            if doc_count == 0:
                st.info("""
                üí° **Getting Started:**
                1. **Upload Documents:** Go to "üìÅ Upload Documents" tab and upload PDF, DOCX, or TXT files
                2. **Enable Web Search:** Toggle "ƒ∞nternette Ara" above for access to current web information  
                3. **Ask Questions:** Return here to ask about your documents or current topics
                4. **Get Smart Answers:** AI will analyze your documents and/or search the web for comprehensive responses
                """)
            elif not st.session_state.chat_history:
                web_status = "üåê Web search enabled" if st.session_state.get('web_search_enabled', False) else "üìÑ Document-only mode"
                st.info(f"""
                üí° **Ready to Chat!**
                
                **Status:** {doc_count} document(s) loaded | {web_status}
                
                **Document Questions:**
                - "What are the main topics in my documents?"
                - "Summarize key findings"
                - "Compare different approaches in the files"
                
                **Web Search Questions** (if enabled):
                - "What are the latest developments in [topic]?"
                - "Find current information about [company/person]"
                - "Compare my document findings with current market trends"
                """)
    
    except Exception as e:
        logger.exception(f"Critical error: {e}")
        st.error(f"‚ùå **Critical Application Error**: {str(e)}")
        st.stop()

if __name__ == "__main__":
    main()